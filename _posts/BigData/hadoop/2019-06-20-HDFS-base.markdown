---
layout:     post
title:      "HDFS基础"
subtitle:   " \"HDFS is the primary distributed storage used by Hadoop applications. \""
date:       2019-06-20 02:30:00
author:     "YaPi"
header-img: "img/bigData/hadoop-logo.jpg"
tags:
    - 大数据
---

### HDFS基础
##### Hadoop Distributed File System
- 分布式文件系统
- 运行在 commodity hardware
- fault-tolerant 高容错
- 部署在廉价的(low-cost hardware)的机器上
- 高吞吐量 high throughput
- 适用于大数据量

##### 普通文件系统

- 目录结构
- 存在的是文件或文件夹
- 对外提供服务: 创建、修改、删除、查看、移动

##### 普通文件系统 VS 分布式文件系统

- 单机  集群
- 分布式文件系统能够横跨N个机器


#### HDFS前提和设计目标

- Hardware Failure(硬件错误) 是很常见的
    1. 每个机器只存储文件的部分数据,blocksize=128M,block存放在不同的机器上来容错
- Streaming Data Access 流式数据访问
- Large Data Sets 大规模数据集
- Moving Computation is Cheaper than Moving Data 移动计算比移动数据更划算

#### HDFS 架构

##### NameNode and DataNodes

- 主从架构
1. 主节点管理NameSpace(这里可以理解为整个文件系统)
2. 主节点提供对客户端的访问 (对文件的CRUD及管理文件与DataNode之间的映射)
3. 数据节点提供数据存储功能
4. 一个文件被拆分为多个block,存在一系列的DataNode上,用于容错

- 副本机制
1. 讲一个大文件氛分为多个block,除了最后一个，每个块大小相同
2. 文件的副本数量可以在创建的时候指定

######  副本摆放策略

默认 相同rack内文件传输就会快一些

1. 本rack的一个节点上
2. 另外一个rack的节点上
3. 与2相同的rack的另外一个节点上

还有其他的方式版本不同可能不同
1. 本rack的一个节点上
2. 本rack的另一个节点上
3. 另外一个rack的节点上

#### 客户端写数据到HDFS的流程

1. 客户端发起上传文件命令
2. namenode判断文件是否存在,并返回是否能写的回复,同时,判断文件的大小返回客户端写入到哪几个datanode
3. 根据文件的大小拆分不同的block
4. 根据返回的datanode的数据传输链接,建立连接(连接其中一个datanode,由datanode再去和其他datanode建立连接?)
5. 按顺序传输block到datanode
6. 通知namenode传输完成,记录元数据信息,名称、block存放的地方等信息


#### 读数据流程

1. 客户端发起读请求到namenode
2. namenode判断是否存在等,若存在,返回文件的元数据信息(文件名称、block的信息)
3. 按顺序发送到datanode读取数据
4. 组装,返回

#### HDFS的元数据管理

- 元数据 : 其实就是HDFS的目录结构以及每个BLOCK信息(id,副本系数,block存在什么地方)
- 存在什么地方: 对应配置 hadoop.tmp.dir 目录下的name文件下面 fsimage文件中
- 文件新增、修改、删除等操作不能直接每次都操作fsimage,文件io很浪费性能
- checkPoint

```
1. 内存中有一个树形结构
2. 新增或修改直接操作内存
3. 定期将内存中的数据写入到fsimage文件中
4. 在未同步到磁盘的时间过程中,将所有对磁盘的操作命令放在edits日志当中
5. 新进程 - Secondary namenode 将fsimage加载磁盘数据到内存中,
然后将edits的操作都作用到内存中去,然后新生成的fsimage替换调原来的fsimage数据。
```

#### 安全模式

namenode在启动过程中,会先进入一个safemode 的状态,暂时是不能进行读写操作的,会接受所有datanode接收到block的信息(大概30s时间),
当检查到所有的block的块的副本数量是配置的最少数量时,就退出安全模式

#### HDFS 命令行操作

```
 hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
        [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-x] <path> ...]
        [-cp [-f] [-p | -p[topax]] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-x] <path> ...]
        [-expunge]
        [-find <path> ... <expression> ...]
        [-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] <src> <localdst>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] <file>]
        [-test -[defsz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touchz <path> ...]
        [-usage [cmd ...]]
```

- hadoop fs -ls/ 查看文件列表
- 上传文件到HDFS
    1. hadoop fs -copyFromLocal LICENSE.txt / 上传文件
    2. hadoop fs -moveFromLocal LICENSE.txt / 上传文件,本地的就没了
    3. hadoop fs -put ReadMe.md / 将ReadMe.md文件上传到HDFS的根目录
- 下载文件
    1.  hadoop fs -get /ReadMe.md
    2.  hadoop fs -getmerge /hdfs-test ./t.txt 把几个文件和成一个文件下载
- hadoop fs -cat /ReadMe.md 读取根目录下的ReadMe.md文件(/ 不能少)
- hadoop fs -text /ReadMe.md 读取
- hadoop fs -mkdir /hdfs-test 创建目录
- hadoop fs -mv /ReadMe.md /hdfs-test
- hadoop fs -cp /hdfs-test/README.txt /hdfs-test/ReadMe.txt_back 拷贝文件
- hadoop fs -rm / 删除单个文件
- hadoop fs -rmdir /hdfs-test 删除不为空的目录
- hadoop fs -rmr /hdfs-test = hadoop fs -rm -rf /hdfs-test 删除目录
- hadoop fs -ls -R /hdfsapi/test 递归展示下面所有文件及文件夹
- hadoop fs -ls  == hadoop fs -ls /usr/hadoop/
- HDFS存储扩展
    1. 1file ==> 1...n block ==> 存放在不同的节点上
    2. 文件存放目录(/home/hadoop/app/tmp/dfs/data/current/BP-1458152468-127.0.0.1-1559113660087/current/finalized/subdir0/subdir0)
    3. 将所有的块按照顺序打包下来 cat block001 testfile, cat blockoo1 testfile...   后续testfile是一个完整的文件
